import os
import subprocess
import serial
import time
import cv2
import numpy as np
import mediapipe as mp
import tensorflow as tf
from collections import deque

# ---- CONFIGURATION ----
BT_MAC_ADDR = "00:22:09:30:A9:5B"
RFCOMM_PORT = "/dev/rfcomm0"
BAUD_RATE = 9600
MODEL_PATH = "best_model_fold1.tflite"
COMMAND_MAP = {'forward': 'U', 'left': 'L', 'right': 'R', 'close': 'S'}
IMAGE_SIZE = 224
RIGHT_EYE_LANDMARKS = [33, 133]

# ---- BLUETOOTH CONNECTION LOOP ----
bt = None
print("ðŸ”„ Attempting to bind and confirm Bluetooth connection...")

while True:
    subprocess.call(["sudo", "rfcomm", "release", RFCOMM_PORT],
                    stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    time.sleep(1)

    try:
        subprocess.check_call(["sudo", "rfcomm", "bind", RFCOMM_PORT, BT_MAC_ADDR],
                              stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        time.sleep(2)
    except subprocess.CalledProcessError:
        print("âŒ Failed to bind rfcomm... retrying in 3s")
        time.sleep(3)
        continue

    try:
        bt = serial.Serial(RFCOMM_PORT, BAUD_RATE, timeout=3)
        time.sleep(1)
        print("ðŸ“¡ Sending handshake to test connection...")
        bt.write(b'ping\n')

        response = bt.read(1)
        if response:
            print("âœ… Bluetooth module responded. Connected!")
            break
        else:
            print("âŒ No response from module... retrying in 3s")
            bt.close()
            bt = None
            time.sleep(3)

    except Exception as e:
        print(f"âŒ Serial error: {e} ... retrying in 3s")
        if bt:
            bt.close()
        bt = None
        time.sleep(3)

# ---- Load TFLite Model ----
interpreter = tf.lite.Interpreter(model_path=MODEL_PATH)
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
class_names = ['close', 'forward', 'left', 'right']

# ---- Gaze Detection Setup ----
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, refine_landmarks=True)
PREDICTION_HISTORY = deque(maxlen=7)
last_command = None

# ---- Load Logo ----
logo = cv2.imread("D:/Mobilenetv2_16-Persons-18-5/Gaze_Logo.png", cv2.IMREAD_UNCHANGED)
if logo is not None:
    logo = cv2.resize(logo, (180, 90))

# ---- Start Camera ----
cap = cv2.VideoCapture(0)
print("ðŸŽ¥ Gaze detection started. Press 'q' to quit.")

# ---- FPS & Control Toggling ----
frame_count = 0
start_time = time.time()
close_frame_counter = 0
control_enabled = True

try:
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = face_mesh.process(rgb)

        label = "Detecting..."
        confidence = 0.0

        if results.multi_face_landmarks:
            for face_landmarks in results.multi_face_landmarks:
                h, w, _ = frame.shape
                x_coords = [int(face_landmarks.landmark[i].x * w) for i in RIGHT_EYE_LANDMARKS]
                y_coords = [int(face_landmarks.landmark[i].y * h) for i in RIGHT_EYE_LANDMARKS]
                x1, y1 = max(0, min(x_coords) - 30), max(0, min(y_coords) - 30)
                x2, y2 = min(w, max(x_coords) + 30), min(h, max(y_coords) + 30)

                eye_crop = frame[y1:y2, x1:x2]
                if eye_crop.shape[0] > 0 and eye_crop.shape[1] > 0:
                    resized = cv2.resize(eye_crop, (IMAGE_SIZE, IMAGE_SIZE))
                    normalized = resized.astype("float32") / 255.0
                    input_data = np.expand_dims(normalized, axis=0)

                    interpreter.set_tensor(input_details[0]['index'], input_data)
                    interpreter.invoke()
                    output_data = interpreter.get_tensor(output_details[0]['index'])

                    pred_idx = np.argmax(output_data[0])
                    current_confidence = output_data[0][pred_idx]
                    PREDICTION_HISTORY.append((pred_idx, current_confidence))

                    all_indices = [idx for idx, conf in PREDICTION_HISTORY]
                    counts = np.bincount(all_indices)
                    smooth_idx = np.argmax(counts)
                    smooth_confidence = np.mean([conf for idx, conf in PREDICTION_HISTORY if idx == smooth_idx])
                    label = f"{class_names[smooth_idx]} ({smooth_confidence:.2f})"

                    current_class = class_names[smooth_idx]

                    # === Toggle control on 60 consecutive "close" detections ===
                    if current_class == "close":
                        close_frame_counter += 1
                    else:
                        close_frame_counter = 0

                    if close_frame_counter >= 60:
                        control_enabled = not control_enabled
                        print(f"ðŸš¦ Control state toggled: {'DISABLED' if not control_enabled else 'ENABLED'}")
                        close_frame_counter = 0

                    current_command = COMMAND_MAP.get(current_class)
                    if control_enabled and current_command and current_command != last_command:
                        try:
                            bt.write(current_command.encode())
                            print(f"ðŸ“¡ Sent: {current_command} ({current_class})")
                            last_command = current_command
                        except Exception as e:
                            print(f"âŒ Bluetooth write error: {e}")

                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)

        # === Build Right Panel ===
        right_panel_width = 350
        frame_h, frame_w = frame.shape[:2]
        side_panel = np.ones((frame_h, right_panel_width, 3), dtype=np.uint8) * 255

        control_text = f"CONTROL: {'ON' if control_enabled else 'OFF'}"
        cv2.putText(side_panel, control_text, (20, 80), cv2.FONT_HERSHEY_SIMPLEX, 1.2,
                    (0, 200, 0) if control_enabled else (0, 0, 255), 3)

        cv2.putText(side_panel, f"DIRECTION:", (20, 160), cv2.FONT_HERSHEY_SIMPLEX, 1.1, (0, 0, 0), 2)
        cv2.putText(side_panel, f"{label.upper()}", (20, 210), cv2.FONT_HERSHEY_SIMPLEX, 1.1, (0, 0, 0), 2)

        if logo is not None:
            lh, lw = logo.shape[:2]
            lx, ly = 85, frame_h - lh - 40
            if logo.shape[2] == 4:
                for c in range(3):
                    side_panel[ly:ly+lh, lx:lx+lw, c] = (
                        logo[:, :, c] * (logo[:, :, 3] / 255.0) +
                        side_panel[ly:ly+lh, lx:lx+lw, c] * (1.0 - logo[:, :, 3] / 255.0)
                    )
            else:
                side_panel[ly:ly+lh, lx:lx+lw] = logo

        # === Final UI Merge ===
        combined = np.hstack((frame, side_panel))
        cv2.imshow("Gaze Control UI", combined)

        # === FPS Calculation ===
        frame_count += 1
        elapsed_time = time.time() - start_time
        if elapsed_time >= 1.0:
            fps = frame_count / elapsed_time
            print(f"ðŸ“ˆ FPS: {fps:.2f}")
            frame_count = 0
            start_time = time.time()

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

except KeyboardInterrupt:
    print("ðŸ‘‹ Interrupted by user.")

finally:
    cap.release()
    cv2.destroyAllWindows()
    if bt:
        bt.close()
    print("ðŸ”š Program ended. Bluetooth closed.")
