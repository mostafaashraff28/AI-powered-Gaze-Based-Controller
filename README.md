# AI-powered-Gaze-Based-Controller
Real-time gaze-controlled robotic car using TensorFlow Lite, MediaPipe, and Raspberry Pi. Detects right-eye movements and converts them into motion commands[Close, left, right, forward] for hands-free navigation. Includes dataset capture, model training (10-fold CV), and deployment code.
Absolutely! Here's your **final polished `README.md`** file â€” fully structured, professional, and ready to publish with your GitHub repository:

---


https://github.com/user-attachments/assets/2ff1b342-abd4-442a-9e50-d316b50129f4


```markdown
# ğŸ‘ï¸ AI-Powered Gaze-Based Robotic Car

A real-time gaze-controlled robotic system designed to assist individuals with motor impairments by allowing them to control a robotic vehicle using only eye movements. This project demonstrates the practical integration of computer vision, deep learning, and embedded systems to create accessible hands-free control solutions.

---

## ğŸ“ Graduation Project â€“ AAST College of Engineering

This project was developed as part of my Bachelor's degree in Computer Engineering at the **Arab Academy for Science, Technology and Maritime Transport (AAST)**. It highlights my passion for building inclusive technologies that make a meaningful difference in real-world accessibility.

---

## ğŸ” Project Overview

| Feature                         | Description                                                  |
|---------------------------------|--------------------------------------------------------------|
| **Input**                       | Real-time video from Raspberry Pi camera                     |
| **Gaze Detection**              | Right-eye tracking via MediaPipe FaceMesh                    |
| **Model Architecture**          | MobileNetV2 + custom classification head                     |
| **Deployment**                  | TensorFlow Lite model on Raspberry Pi 4                      |
| **Output**                      | Directional commands via Bluetooth (forward, left, right)    |

---

## ğŸ¯ Objectives

- **Develop** a hands-free robotic control system using gaze.
- **Enhance** detection accuracy with landmark filtering and smoothing.
- **Maintain** real-time performance on edge devices like Raspberry Pi 4.
- **Ensure** a calibration-free, user-friendly experience.
- **Validate** performance using 10-fold subject-independent cross-validation.

---

## ğŸ§  Technologies Used

- **Python 3.9**
- **TensorFlow 2.x + TensorFlow Lite**
- **OpenCV**
- **MediaPipe**
- **Raspberry Pi 4 (Linux)**
- **Bluetooth Serial Communication (`pyserial`)**
- **Keras, NumPy, Matplotlib, Seaborn**

---

## ğŸ“ Project Structure

```

Gaze-Controlled-Robotic-Car/
â”œâ”€â”€ dataset\_collection/
â”‚   â””â”€â”€ RIGHT_EYE.py                  # Eye image capture using MediaPipe 4 classes[Forward, close, left, right]
â”‚
â”œâ”€â”€ model\_training/
â”‚   â””â”€â”€ 20-10kfolds.ipynb             # Model training with 10-fold CV (MobileNetV2)
â”‚
â”œâ”€â”€ model\_integration/
â”‚   â””â”€â”€ Full\_code\_GUI\_DONE.py        # Real-time inference + Bluetooth control
â”‚
â”œâ”€â”€ best\_model\_fold1.tflite          # Optimized model for deployment
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ README.md                        # Full documentation
â””â”€â”€ LICENSE                          # MIT License

````

---

## ğŸš€ How to Use

### 1. Collect Right-Eye Dataset
Capture labeled eye images with:
```bash
python dataset_collection/RIGHT_EYE.py
````

* Change `CLASS_LABEL` and `PERSON_ID` inside the script.

---

### 2. Train the Model

Run:

```bash
model_training/20-10kfolds.ipynb
```

* Trains MobileNetV2 in two phases (frozen head â†’ fine-tuning)
* Uses cosine decay and early stopping
* Outputs `.keras` and `.tflite` models

---

### 3. Run Real-Time Controller

On Raspberry Pi (with camera + Bluetooth):

```bash
python model_integration/Full_code_GUI_DONE.py
```

* Loads `.tflite` model
* Detects right eye â†’ predicts gaze direction
* Sends control command to robot via `/dev/rfcomm0`

---

## ğŸ“Š Model Performance

| Metric               | Result                       |
| -------------------- | ---------------------------- |
| Input Image Size     | 224 Ã— 224 (right eye)        |
| Accuracy (Best Fold) | 98.9%                        |
| Evaluation           | 10-fold cross-validation     |
| Real-Time FPS        | \~8â€“10 frames per second     |
| Inference Speed      | \~90â€“100ms per frame (RPi 4) |

---

## ğŸ”¬ Model Training Details

* **Base Model**: MobileNetV2 pretrained on ImageNet
* **Classifier Head**: GlobalAveragePooling â†’ Dropout â†’ Dense(softmax)
* **Loss Function**: Categorical Crossentropy with label smoothing (0.1)
* **Optimizer**: Adam
* **Learning Rate**:

  * Phase 1: Cosine decay (1e-3 â†’ 1e-6)
  * Phase 2: Fixed (2e-5)
* **Data Augmentation**:

  * Rotation Â±15Â°
  * Zoom 0.8â€“1.2
  * Brightness 0.6â€“1.4
  * Width/Height shift Â±10%
* **Evaluation Metrics**: Accuracy, F1-score, Confusion Matrix

---

## ğŸ“¦ Installation

Install Python dependencies:

```bash
pip install -r requirements.txt
```

`requirements.txt`:

```
tensorflow
opencv-python
mediapipe
numpy
pyserial
matplotlib
seaborn
scikit-learn
```

---

## ğŸ§‘â€ğŸ’» Author

**\Mostafa Ashraf Mostafa]**
B.Sc. in Computer Engineering â€“ AAST
ğŸ“« Email: \[[mostefaashraf@gmail.com]]
ğŸ”— LinkedIn: \[www.linkedin.com/in/mostafa-ashraf-074792324]
ğŸŒ Portfolio: \[https://mostafa.mikawi.org/]

> Open to roles in:
> âš™ï¸ Computer Vision â€¢ ğŸ§  Embedded AI â€¢ ğŸ¤– Robotics â€¢ â™¿ Assistive Technologies

---

## ğŸ“½ï¸ Demo Video

ğŸ¥ Watch the system in action:
ğŸ”— \[www.linkedin.com/in/mostafa-ashraf-074792324]

---

## ğŸ“œ License

This project is licensed under the [MIT License](LICENSE).
You are free to use, modify, and distribute it with proper attribution.

---

## ğŸ™ Acknowledgments

Special thanks to my academic supervisors, peers, and everyone who supported me in this journey. This project is dedicated to using AI for good â€” empowering those who need it most.

---

